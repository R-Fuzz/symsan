import atexit

from agent import *
from defs import TaintFlag
from learner import BasicQLearner

class ExploitAgent(Agent):

    def __init__(self, config: Config):
        super().__init__(config)
        # TODO: remove assertion after testing
        assert config.mazerunner_dir is not None
        self.model = RLModel(config)
        self.model.load()
        atexit.register(self.model.save)
        self.target_sa = None
        self.learner = BasicQLearner(self.model, config.discount_factor, config.learning_rate)

    def handle_new_state(self, msg, action):
        self.update_curr_state(msg, action)
        self.episode.append(self.curr_state.serialize())
        curr_sa = self.curr_state.state + (self.curr_state.action, )
        if curr_sa == self.target_sa:
            self.target_sa = None

    def is_interesting_branch(self):
        if self.target_sa:
            return False
        reversed_sa = self.curr_state.compute_reversed_sa()
        if reversed_sa in self.model.unreachable_sa:
            return False
        curr_sa = self.curr_state.state + (self.curr_state.action, )
        if (reversed_sa not in self.model.visited_sa
            and curr_sa not in self.model.visited_sa):
            return False
        if self._greedy_policy() == self.curr_state.action:
            return False
        # TODO: also explore the deep unvisited sa, controlled by epsilon
        return True

    def _greedy_policy(self):
        curr_state_taken = self.model.Q_lookup(self.curr_state.state +(1,))
        curr_state_not_taken = self.model.Q_lookup(self.curr_state.state +(0,))
        if curr_state_taken > curr_state_not_taken:
            return 1
        elif curr_state_taken < curr_state_not_taken:
            return 0
        else:
            return self.curr_state.action
